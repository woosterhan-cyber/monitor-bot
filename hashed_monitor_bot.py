import os
import json
import hashlib
import time
import requests
import feedparser
import gspread
from google.oauth2.service_account import Credentials
from dateutil import parser as dateparser
from datetime import datetime, timezone, timedelta
from zoneinfo import ZoneInfo
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode


# ============================================================
# CONFIG
# ============================================================
SLACK_BOT_TOKEN = os.getenv("SLACK_BOT_TOKEN", "").strip()
SLACK_CHANNEL = os.getenv("SLACK_CHANNEL", "").strip()  # ì±„ë„ ID(C...) ê¶Œì¥

GOOGLE_SHEET_ID = os.getenv("GOOGLE_SHEET_ID", "").strip()
GOOGLE_SERVICE_ACCOUNT_JSON = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON", "").strip()

# (í•„ìš”í•˜ë©´ í‚¤ì›Œë“œ í™•ì¥ ê°€ëŠ¥)
BASE_KEYWORDS = ["Hashed", "í•´ì‹œë“œ"]

# Slack í­ì£¼ ë°©ì§€
MAX_SLACK_ALERTS = 10

# ì‹œíŠ¸ì—ì„œ ì´ë¯¸ ë³¸ ID ì½ì–´ì˜¤ëŠ” ìµœëŒ€ ê°œìˆ˜ (ë„ˆë¬´ ì»¤ì§ˆ ê²½ìš° ëŒ€ë¹„)
SHEET_ID_LOAD_LIMIT = 8000

# ì˜¤ë˜ëœ ê¸°ì‚¬ ë°©ì§€ ì•ˆì „ì¥ì¹˜ (ë³´í—˜)
MAX_LOOKBACK_DAYS = 7

# ============================================================
# UTILS
# ============================================================
def normalize_url(url: str) -> str:
    """URLì—ì„œ íŠ¸ë˜í‚¹ íŒŒë¼ë¯¸í„° ë“±ì„ ì œê±°í•´ id ì•ˆì •ì„± ê°œì„ ."""
    if not url:
        return url
    url = url.strip()
    p = urlparse(url)

    # fragment ì œê±°
    p = p._replace(fragment="")

    # queryì—ì„œ íŠ¸ë˜í‚¹ ì œê±°
    q = parse_qsl(p.query, keep_blank_values=True)
    filtered = []
    for k, v in q:
        lk = k.lower()
        if lk.startswith("utm_"):
            continue
        if lk in ("fbclid", "gclid", "mc_cid", "mc_eid"):
            continue
        filtered.append((k, v))
    new_query = urlencode(filtered, doseq=True)
    p = p._replace(query=new_query)

    # í˜¸ìŠ¤íŠ¸ ì†Œë¬¸ì
    p = p._replace(netloc=p.netloc.lower())
    return urlunparse(p)


def today_midnight_kst_utc() -> datetime:
    """ì˜¤ëŠ˜ 0:00(KST)ë¥¼ UTCë¡œ ë³€í™˜í•´ ë°˜í™˜."""
    kst = ZoneInfo("Asia/Seoul")
    now_kst = datetime.now(kst)
    midnight_kst = now_kst.replace(hour=0, minute=0, second=0, microsecond=0)
    return midnight_kst.astimezone(timezone.utc)


def make_id(source: str, url: str) -> str:
    """ê¸°ì‚¬ ì¤‘ë³µ íŒì •ìš© ID."""
    url = normalize_url(url)
    raw = f"{source}|{url}".encode("utf-8")
    return hashlib.sha256(raw).hexdigest()


def safe_parse_dt(value: str):
    """
    published_at íŒŒì‹±.
    IMPORTANT:
      - íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜ (ì ˆëŒ€ nowë¡œ ëŒ€ì²´í•˜ì§€ ì•ŠìŒ)
      - ì´ê²Œ 2023ë…„ ê¸°ì‚¬(í˜¹ì€ ì´ìƒí•œ ê¸°ì‚¬)ê°€ ìƒˆ ê¸°ì‚¬ë¡œ íŒì •ë˜ëŠ” ë²„ê·¸ë¥¼ ë§‰ìŒ
    """
    if not value:
        return None
    try:
        dt = dateparser.parse(value)
        if not dt:
            return None
        if not dt.tzinfo:
            # timezone ì •ë³´ê°€ ì—†ìœ¼ë©´ UTCë¡œ ê°„ì£¼
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None


# ============================================================
# SLACK
# ============================================================
def slack_headers():
    if not SLACK_BOT_TOKEN.startswith("xoxb-"):
        raise RuntimeError("SLACK_BOT_TOKEN is missing or invalid. (should start with xoxb-...)")
    return {
        "Authorization": f"Bearer {SLACK_BOT_TOKEN}",
        "Content-Type": "application/json; charset=utf-8"
    }


def slack_post_with_retry(payload, retries=3):
    """Slack rate limit ëŒ€ë¹„ ì¬ì‹œë„."""
    for attempt in range(1, retries + 1):
        r = requests.post(
            "https://slack.com/api/chat.postMessage",
            headers=slack_headers(),
            json=payload,
            timeout=15
        )
        data = r.json()
        if data.get("ok"):
            return True

        err = data.get("error")
        if err == "rate_limited":
            time.sleep(2 * attempt)
            continue

        print("[Slack] post failed:", err)
        return False
    return False


def slack_post_mention(channel_id: str, mention: dict):
    title = mention["title"]
    url = mention["url"]
    source = mention["source"]
    published = mention["published_at"]

    blocks = [
        {"type": "header", "text": {"type": "plain_text", "text": "ğŸŸ£ Hashed Mentions Alert", "emoji": True}},
        {"type": "divider"},
        {"type": "section",
         "text": {"type": "mrkdwn",
                  "text": f"*<{url}|{title}>*\n\n*Source:* `{source}`\n*Published:* `{published}`"}},
        {"type": "divider"},
        {"type": "context", "elements": [{"type": "mrkdwn", "text": "ìë™ ëª¨ë‹ˆí„°ë§ ë´‡ (Google News RSS + GDELT)"}]}
    ]

    payload = {"channel": channel_id, "text": f"[{source}] {title}", "blocks": blocks}
    ok = slack_post_with_retry(payload)
    if not ok:
        raise RuntimeError("Slack chat.postMessage failed after retries.")


def slack_post_digest(channel_id: str, mentions: list):
    """Slack í­ì£¼ ë°©ì§€: ë‚¨ì€ í•­ëª©ì€ digest 1ë²ˆìœ¼ë¡œ ìš”ì•½."""
    if not mentions:
        return

    lines = "\n".join([f"â€¢ <{m['url']}|{m['title']}>" for m in mentions[:20]])
    extra = len(mentions) - min(len(mentions), 20)
    if extra > 0:
        lines += f"\nâ€¦ and {extra} more."

    payload = {
        "channel": channel_id,
        "text": f"ğŸ§¾ Hashed Mentions Digest: {len(mentions)} more",
        "blocks": [
            {"type": "header", "text": {"type": "plain_text", "text": f"ğŸ§¾ Digest: {len(mentions)} more mentions", "emoji": True}},
            {"type": "section", "text": {"type": "mrkdwn", "text": lines}},
        ]
    }
    slack_post_with_retry(payload)


# ============================================================
# GOOGLE SHEETS (DB)
# ============================================================
def get_gspread_client():
    if not GOOGLE_SERVICE_ACCOUNT_JSON:
        raise RuntimeError("GOOGLE_SERVICE_ACCOUNT_JSON is missing.")
    if not GOOGLE_SHEET_ID:
        raise RuntimeError("GOOGLE_SHEET_ID is missing.")
    info = json.loads(GOOGLE_SERVICE_ACCOUNT_JSON)
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = Credentials.from_service_account_info(info, scopes=scopes)
    return gspread.authorize(creds)


def get_worksheets():
    """sheet1(ê¸°ì‚¬ ì €ì¥) + meta(since ì €ì¥)"""
    gc = get_gspread_client()
    sh = gc.open_by_key(GOOGLE_SHEET_ID)

    ws = sh.sheet1
    try:
        meta_ws = sh.worksheet("meta")
    except Exception:
        raise RuntimeError("Worksheet 'meta' not found. Please create a sheet tab named 'meta' with key/value rows.")
    return ws, meta_ws


def sheet_get_existing_ids(ws, limit=SHEET_ID_LOAD_LIMIT):
    """Aì—´(id)ì„ ì½ì–´ì„œ ì´ë¯¸ ë³¸ ê¸°ì‚¬ set êµ¬ì„±."""
    col = ws.col_values(1)
    if not col:
        return set()
    ids = col[1:]  # header ì œì™¸
    if len(ids) > limit:
        ids = ids[-limit:]
    return set(ids)


def sheet_append_rows(ws, rows):
    """rows: [id, fetched_at, published_at, source, title, url]"""
    if not rows:
        return
    ws.append_rows(rows, value_input_option="RAW")


def meta_get_since(meta_ws):
    values = meta_ws.get_all_values()
    for row in values[1:]:
        if len(row) >= 2 and row[0] == "since":
            return row[1].strip() if row[1] else None
    return None


def meta_set_since(meta_ws, iso_time):
    values = meta_ws.get_all_values()
    for i, row in enumerate(values[1:], start=2):
        if len(row) >= 1 and row[0] == "since":
            meta_ws.update_cell(i, 2, iso_time)
            return
    meta_ws.append_row(["since", iso_time], value_input_option="RAW")


# ============================================================
# SOURCES
# ============================================================
def fetch_google_news_rss(query: str):
    url = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(url)

    results = []
    for entry in feed.entries:
        title = entry.title
        link = entry.link
        published = entry.get("published")

        published_dt = safe_parse_dt(published) if published else None
        if not published_dt:
            # âœ… published_atì´ ì—†ìœ¼ë©´ ì´ ì†ŒìŠ¤ì—ì„œëŠ” ìŠ¤í‚µí•˜ëŠ”ê²Œ ì•ˆì „
            # (ì˜¤ë˜ëœ ê¸°ì‚¬/ê¹¨ì§„ ê¸°ì‚¬ë“¤ì´ nowë¡œ ê°„ì£¼ë˜ëŠ” ë¬¸ì œ ë°©ì§€)
            continue

        results.append({
            "source": "GoogleNewsRSS",
            "title": title,
            "url": link,
            "published_at": published_dt.isoformat(),
        })
    return results


def fetch_gdelt(query: str, max_records=50, retries=3):
    """
    GDELTëŠ” HTML ì˜¤ë¥˜ë¥¼ ì£¼ê¸°ë„ í•˜ë¯€ë¡œ ë°©ì–´ + ì¬ì‹œë„.
    ì‹¤íŒ¨í•´ë„ [] ë°˜í™˜(ì „ì²´ ë´‡ì€ ê³„ì† ë™ì‘).
    """
    url = "https://api.gdeltproject.org/api/v2/doc/doc"
    params = {
        "query": query,
        "mode": "ArtList",
        "format": "json",
        "maxrecords": max_records,
        "sort": "HybridRel"
    }
    headers = {"User-Agent": "HashedMonitorBot/1.0"}

    last_err = None
    for attempt in range(1, retries + 1):
        try:
            r = requests.get(url, params=params, headers=headers, timeout=20)

            if r.status_code != 200:
                last_err = f"GDELT HTTP {r.status_code}"
                print(f"[GDELT] attempt {attempt}/{retries} failed: {last_err}")
                continue

            if not r.text or len(r.text.strip()) == 0:
                last_err = "GDELT empty response"
                print(f"[GDELT] attempt {attempt}/{retries} failed: {last_err}")
                continue

            ctype = r.headers.get("Content-Type", "")
            if "application/json" not in ctype:
                last_err = f"GDELT non-json content-type: {ctype}"
                print(f"[GDELT] attempt {attempt}/{retries} failed: {last_err}")
                print("[GDELT] response head:", r.text[:200])
                continue

            data = r.json()
            results = []
            for item in data.get("articles", []):
                title = item.get("title", "")
                link = item.get("url", "")
                seendate = item.get("seendate")  # yyyymmddHHMMSS

                published_dt = None
                if seendate:
                    try:
                        published_dt = datetime.strptime(seendate, "%Y%m%d%H%M%S").replace(tzinfo=timezone.utc)
                    except Exception:
                        published_dt = None

                # âœ… íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ìŠ¤í‚µ (now ëŒ€ì²´ ì ˆëŒ€ ê¸ˆì§€)
                if not published_dt:
                    continue

                results.append({
                    "source": "GDELT",
                    "title": title,
                    "url": link,
                    "published_at": published_dt.isoformat(),
                })
            return results

        except Exception as e:
            last_err = str(e)
            print(f"[GDELT] attempt {attempt}/{retries} exception: {last_err}")

    print(f"[GDELT] giving up after {retries} attempts. last_err={last_err}")
    return []


# ============================================================
# MAIN
# ============================================================
def run():
    if not SLACK_CHANNEL:
        raise RuntimeError("SLACK_CHANNEL is missing. Use channel ID like C0123... recommended.")

    now_utc = datetime.now(timezone.utc)
    fetched_at = now_utc.isoformat()

    midnight_utc = today_midnight_kst_utc()
    too_old_cutoff = now_utc - timedelta(days=MAX_LOOKBACK_DAYS)
    future_cutoff = now_utc + timedelta(minutes=5)

    # 1) Sheet ì—°ê²°
    ws, meta_ws = get_worksheets()

    # 2) since ì½ê¸°
    since_str = meta_get_since(meta_ws)

    if not since_str:
        # âœ… ì²« ì‹¤í–‰: 'ì§€ê¸ˆë¶€í„° ì‹œì‘'
        meta_set_since(meta_ws, now_utc.isoformat())
        print("First run: since initialized to now. No notifications this run.")
        return

    since_dt = safe_parse_dt(since_str)
    if not since_dt:
        # meta sinceê°€ ê¹¨ì§„ ê²½ìš°ì—ë„ ì•ˆì „í•˜ê²Œ nowë¡œ ë¦¬ì…‹
        meta_set_since(meta_ws, now_utc.isoformat())
        print("since value invalid â†’ reset to now, skipping this run.")
        return

    # âœ… ì˜¤ëŠ˜ 0:00(KST) ì´ì „ì€ ë¬´ì¡°ê±´ ë°©ì§€
    if since_dt < midnight_utc:
        since_dt = midnight_utc

    # 3) Query (GDELT OR ê´„í˜¸ ê·œì¹™ ì¤€ìˆ˜)
    google_query = '("Hashed" OR "í•´ì‹œë“œ")'
    gdelt_query = '("Hashed" OR "í•´ì‹œë“œ")'

    # 4) Fetch
    all_results = []
    all_results += fetch_google_news_rss(google_query)
    all_results += fetch_gdelt(gdelt_query)

    # 5) ë‚ ì§œ í•„í„° (ì—¬ê¸°ê°€ ì´ë²ˆ ë¬¸ì œì˜ í•µì‹¬)
    filtered = []
    for m in all_results:
        pub_dt = safe_parse_dt(m.get("published_at"))

        # âœ… íŒŒì‹± ì‹¤íŒ¨í•œ ê±´ ì ˆëŒ€ nowë¡œ ì²˜ë¦¬í•˜ì§€ ë§ê³  ìŠ¤í‚µ
        if not pub_dt:
            print("[WARN] published_at parse failed â†’ skip:", m.get("source"), m.get("title"))
            continue

        # âœ… ì´ìƒí•˜ê²Œ ì˜¤ë˜ëœ ê¸°ì‚¬/ë¯¸ë˜ ê¸°ì‚¬ ë³´í—˜
        if pub_dt < too_old_cutoff:
            # ì˜ˆ: 2023ë…„ ê¸°ì‚¬ ê°™ì€ ê²ƒë“¤ ê°•ì œ ì°¨ë‹¨
            continue
        if pub_dt > future_cutoff:
            continue

        # âœ… â€œì˜¤ëŠ˜ 0:00 ì´í›„â€ + â€œsince ì´í›„â€ë§Œ
        if pub_dt >= midnight_utc and pub_dt >= since_dt:
            filtered.append(m)

    # 6) ì‹œíŠ¸ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    existing_ids = sheet_get_existing_ids(ws)
    new_mentions = []
    for m in filtered:
        m["url"] = normalize_url(m["url"])
        m_id = make_id(m["source"], m["url"])
        if m_id not in existing_ids:
            m["id"] = m_id
            m["fetched_at"] = fetched_at
            new_mentions.append(m)

    if new_mentions:
        print(f"âœ… New mentions: {len(new_mentions)}")

        # âœ… ì‹œíŠ¸ ì €ì¥ ë¨¼ì €
        rows = []
        for m in new_mentions:
            rows.append([m["id"], m["fetched_at"], m["published_at"], m["source"], m["title"], m["url"]])
        sheet_append_rows(ws, rows)

        # âœ… Slack ì „ì†¡ (ìƒìœ„ Nê°œ ê°œë³„ + ë‚˜ë¨¸ì§€ digest)
        to_send = new_mentions[:MAX_SLACK_ALERTS]
        remaining = new_mentions[MAX_SLACK_ALERTS:]

        for m in to_send:
            slack_post_mention(SLACK_CHANNEL, m)

        if remaining:
            slack_post_digest(SLACK_CHANNEL, remaining)

    else:
        print("No new mentions.")

    # 7) since ê°±ì‹ : ë‹¤ìŒ ì‹¤í–‰ì€ ì´ë²ˆ ì‹¤í–‰ ì´í›„ ê¸°ì‚¬ë§Œ
    meta_set_since(meta_ws, now_utc.isoformat())
    print(f"[meta] since updated to {now_utc.isoformat()}")


if __name__ == "__main__":
    run()


